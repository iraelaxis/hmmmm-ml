# Основные задачи

TODO: краткое описание основных областей

* Классификация

* Регрессия



# Классификация задач по сложности

В оценке сложности указана минимальная граница. Т.е. сколько нужно потратить человеко-часов для создания PoC.




## NLP (обработка естественного языка)

Все нижеописанные области так или иначе опираются на наличие Text corpus по данным темам.

__Abstraction-based summarization.__ - суммаризация текста с перефразированием предложений. На данный момент технически нереализуема. Возможный подход - lstm, attention mechanism. (10\10)


__Extraction-based summarization.__ - суммаризация текста путём выдирания предложений без их изменений. (3\10)

Сама задача основана больше на применении статистических методик и применения готовых датасетов дистанции слов.
Суть текста может передаваться, а может и нет. Сам подход тупиковый по развитию. Реализуется относитально просто.


__Text classification.__ - классификация текста по темам (3-6\10)

Зависит от наличия готовых датасетов. Минимально требуется от ~50000 слов по теме.



__Text clusterization.__ - кластеризация по темам (6/10)

Сложно оценивать качество модели.



__Information extraction.__ - извлечение информации из неформатированного текста (выделить автора, тему, создать структуру) (9/10)


__Sentiment analysis.__ - анализ общего настроения текста (2\10)

Сложность зависит от наличия и качества языкового датасета положительного\отрицательного оттенка слова.



__Синонимайзеры, чат-боты, автоответчики по базе шаблонов__ (4\10)

ML подход лучше не использовать.





## Computer vision

Всё опирается на наличие датасета, для распознения требуется обычно ~10000 изображений по 
Сложность вычисляется исходя из предположения, что 10000 примеров присутствует в выборке.


### Image Recognition

__Определение есть ли [объект] на изображении.__ (2/10)

Вероятностная мера



__Определение если ли [объект №1]...[объект №2]...[объект №n] на изображении.__ (5/10)

В NN подходе могут быть проблемы при обучении сети (по сути - часть задачи классификации)



__Image Segmentation/Object detection.__ найти объект и выделить его границы (9/10)

Высокая сложность в силу проблемной подготовки датасета для обучения



#### Принцип создания DNN моделей\датасета в работе с изображениями:

__[Важные моменты]__

Любая сеть будет работать с изображениями размера максимум 64х64 (обычно 32х32). Если размер больше, то он ресайзится.
Следовательно объект, который необходимо распознать должен занимать собой минимум 70-80% изображения.

Если объект, который надо распознать, занимает ~50% места, то желательно трансформировать изображение так, чтобы он занимал 70-80, так как неизвестно, что из оставшихся 50% будет взято в качестве признаков этого объекта. Например весь датасет состоит из 1000 фотографий различных кошек с облаком в правом верхнем углу. Распознаётся фотография собаки с тем же облаком в правом верхнем углу. При распознании собака будет принята за кошку с 99% вероятностью, посколько последний связи в модели скорее всего будут выглядеть как: наличие массива белых пикселей(облако) -> кошка. Всё это относится к проблеме переобучения (overfitting) и разнообразия сэмплов см. проблемы.


Сложность задачи сегментации в том, что при создании датасета необходимо вручную создать маску формы (очертания) объекта для каждого изображения, либо найти готовый датасет с масками, а их не особо много.

__[Датасет]__

Датасет можно искусственно расширить, используя различные трансформаци (перемещение, увеличение\отдаление, повороты). Фильтры лучше не использовать.

При тренировке можно взять готовые веса, обнулив последний слой и дотренировав на какой-то конкретной задаче.


---

### Video


__Single object tracking.__ - трекинг одного объекта (8-9/10)


__Multi-object tracking.__ - трекинг множества объектов (9-10/10)

Включают в себя 3 задачи - сегментация объекта + идентификация объекта + множественное отслеживание.

Основные проблемы заключаются в установлении связи кадров друг с другом (тот же самый ли объект на кадре n-1 и кадре n), пересечении объектов друг с другом, идентификации неподвижных объектов, определении геометрии при подвижной камере.


__Определение действий объекта, т.е бег/драка/падение.__ (10/10)

Тема модная из-за теоретически допустимой возможности предотвращать преступления, автоматически анализируя видео с камер наблюдения. На данный момент технически нереализуема в прикладном плане (продакшн). Научным работам не верить.



### Time-series (временные ряды)
WIP
TODO: amr\features



### Задачи с прикладной регрессией (цифры итд)
WIP




# Усреднённое распределение времени при разработке

40% - выделение признаков (Feature extraction) и их формирование

20% - само построение модели

40% - тонкая настройка модели

Большинство времени тратится на подготовку данных к отправке в модель.




# Данные

## Признаки

Примерные пути создания признаков.

easy mode: найти признаки -> сформировать в csv таблицу -> отправить в pandas -> скормить модели

normal mode: Добавить отбор признаков (feature selection). Рассмотрение корреляций, вариаций
[NB] даже если признак обладает низкой значимостью (feature importance) выкидывать его может быть плохой идеей (см. проблема новых реальных данных)

hard mode: Генерация признаков. Рассмотрение статистических распределений, привод признаков к (как правило) нормальному распределению. Q–Q plot проверка. Полный перебор.




## Общий принцип подготовки датасета:

Деление данных на две части (train и test)
На train тренируется модель, на test - тестируется. Типичное соотношение - от 0.5\0.5 до 0.8\0.2


# Типичные проблемы

Проблема:
Новые реальные данные (не входящие в изначальную выборку) могут отличаться от исходных, модель их игнорирует.

Решение:
Самое простое - натренировать модель заного, включив в неё новые данные.
Сложнее - аккуратно понизить параметры модели (меньшее количество эпох для NN, ниже learning rate, e.t.c. Да, уменьшится точность, но модель будет охотнее принимать новые данные. Подробнее - уходить в проблему ovefitting. Суть решения - модель меньше основывается на некоторые специфический признаки для исходной выборки и смотрит больше в сторону более абстрактных.

---

Проблема:
Отсутствие некоторых данных (пропуски, N/A)

Решение:
В зависимости от типа модели заполнить пропуски либо средним значением, либо наименее вероятным значением для данных фич (если типичные значения в [-10, 10], то выставить -9999999999, 9999999999)

---





# Основные модели

## Выбор модели (простые)

![ml map](./images/ml_map.png "ml-map")

## Выбор модели (композиции алгоритмов)

Общий принцип: взятие нескольких независимых статистических оценок и устреднение их результатов.

* Random Forest

* XGBoost






# Проверка качества и надёжности модели

Подготовка  
---
**Устранение случайности**

Каждая модель запускается со случайным seed. Для адекватной оценки улучшения по какому-либо параметру, необходимо выставить одинаковый seed на все запуски тестирования.

Установка seed может быть в форме: 

```random.seed(42)```
```random_state=42``` для большинства функций из sklearn

Усреднение рандома - запускать на n-количестве различных известных seed и усреднять их результаты.
+ Перекрёстная проверка (см. дальше)
---


## Доказательство выбора наилучшего параметра модели


#### Выбор наилучших параметров по метрике среднеквадратического отклонения

<http://www.statisticshowto.com/rmse/>

*Показатель рассеивания значений случайной величины относительно её математического ожидания*

*Мера отклонения от истинного результата* 

*Меньше - лучше*


```Python
from sklearn import metrics
import numpy as np
import pandas as pd

# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
from sklearn.linear_model import Lasso, LassoCV

def rmse(y_true, y_pred):
    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))

# создание метрики среднеквадратической ошибки.
rmse_scorer = metrics.make_scorer(rmse, greater_is_better=False)


# датасет с какой-либо регрессией (влияние независимых x_1, x_2, ..., x_n на зависимую Y)
df = pd.read_csv('./train.csv')
y = df['Y']
df.drop('Y', 1, inplace=True)


# настройка alpha параметра

# список возможных alpha (100 равномерно распределённых значений между [0.0007;0.0009])
# из предположения, что данный интервал был выбран из предыдущих итераций поиска с бОльшими интервалами
test_alphas = np.linspace(0.0007, 0.0009, 100)


# наилучшие значения выбираются при помощи перекрёстной проверки
# (демонстрация того, что данный результат не единичная случайность)
model_lasso = LassoCV(alphas = test_alphas, cv=4).fit(df, y)

lasso = Lasso(max_iter=5000)

# verbose = 10... для вывода всех значений (полезно для составления таблицы зависимости точность-параметр)
lasso_gs = model_selection.GridSearchCV(lasso, {'alpha': alphas}, cv=4, scoring=rmse_scorer, verbose=0)

# Наилучшее значение RMSE
print(lasso_gs.best_score_)
# -0.113992634968

# Наилучшая alpha
print(lasso_gs.best_params_)
# {'alpha': 0.00088585858585858582}
```

В явном виде показано, что при alpha = 0.00088585858585858582 достигается наилучшее значение среднеквадратической ошибки.

---

#### Выбор параметров по метрике коэффициента детерминации

<https://people.duke.edu/~rnau/rsquared.htm>

*Доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью*

*Единица минус доля необъяснённой дисперсии* 

*Больше - лучше*


>Коэффициент детерминации для модели с константой принимает значения от 0 до 1. Чем ближе значение коэффициента к 1, тем сильнее зависимость. При оценке регрессионных моделей это интерпретируется как соответствие модели данным. Для приемлемых моделей предполагается, что коэффициент детерминации должен быть хотя бы не меньше 50% (в этом случае коэффициент множественной корреляции превышает по модулю 70%). Модели с коэффициентом детерминации выше 80% можно признать достаточно хорошими (коэффициент корреляции превышает 90%). Равенство коэффициента детерминации единице означает, что объясняемая переменная в точности описывается рассматриваемой моделью. 



```Python
from sklearn.metrics import r2_score
...
r2_scorer = metrics.make_scorer(r2_score, greater_is_better=True)
lasso_gs = model_selection.GridSearchCV(lasso, {'alpha': alphas}, cv=4, scoring=r2_scorer, verbose=0)
...

# Наилучшее значение R2
print(lasso_gs.best_score_)
# 0.918229260125

# Наилучшая alpha
print(lasso_gs.best_params_)
# {'alpha': 0.00085959595959595955}

```

---

```
alpha(RMSE) = 0.00088585858585858582
alpha(R-sq) = 0.00085959595959595955

Усреднённая alpha = 0.00087
```

Более точная настройка проводится на тестовой выборке с учётом точности и отклонений.

TODO: рассмотреть влияние на тестовую выборку

Аналогичным способом подбираются остальные параметры.

---

##### XGBoost
<https://github.com/dmlc/xgboost>

```Python
import xgboost as xgb
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_boston

rng = np.random.RandomState(42)


boston = load_boston()
y = boston['target']
X = boston['data']
xgb_model = xgb.XGBRegressor(random_state=42)
clf = GridSearchCV(xgb_model,
                   {'min_child_weight': np.linspace(0.8, 1.2, 20),
                    'max_depth': [3,4],
                    'n_estimators': range(100, 200, 5)}, verbose=0, cv=4)
clf.fit(X,y)
print(clf.best_score_)
print(clf.best_params_)

# если в явном виде не указывать метрику, то оптимизация параметров идёт по дефолтной для модели
# здесь - точность

# 0.643383034893
# {
# 	'max_depth': 3,
# 	'min_child_weight': 1.0105263157894737, 
# 	'n_estimators': 195
# }
```

```Python
# увеличение количества перебираемых параметров
clf = GridSearchCV(xgb_model,
                   {'learning_rate': np.linspace(0.01, 0.2, 20),
                    'min_child_weight': np.linspace(0.9, 1.2, 20),
                    'max_depth': [3,4],
                    'n_estimators': range(100, 300, 5)}, verbose=0, cv=4)

print(clf.best_score_)
print(clf.best_params_)


# 0.65207399603
# {
#	'learning_rate': 0.089999999999999997, 
#	'max_depth': 3, 
#	'min_child_weight': 0.90000000000000002, 
#	'n_estimators': 280
# }
```


# Продвинутые модели (когда важны +/- 0.001% качества)
WIP

TODO: написать про ансамбли моделей